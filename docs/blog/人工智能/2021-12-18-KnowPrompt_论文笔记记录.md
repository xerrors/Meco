---
title: KnowPrompt è®ºæ–‡ç¬”è®°è®°å½•
date: 2021-12-18 13:33:44
permalink: /know-prompt/
cover: 
tags: 
- Prompt
- äººå·¥æ™ºèƒ½
categories: äººå·¥æ™ºèƒ½
---

# KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction

ä½œè€…: Xiang.Chen
å¯ç”¨é“¾æ¥: https://arxiv.org/abs/2104.07650, https://www.connectedpapers.com/main/3624cbbe08f6bc7979e7403d2b32b70816f810f1/KnowPrompt%3A-Knowledge%20aware-Prompt%20tuning-with-Synergistic-Optimization-for-Relation-Extraction/graph, https://github.com/zjunlp/KnowPrompt
å¹´ä»½: 2021
æ—¥æœŸ: December 18, 2021 11:24 AM
çŠ¶æ€: ç•¥è¯»
è®ºæ–‡æ ‡ç­¾: Prompt, RE

![Untitled](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20220307133634.png)

æœ€è¿‘ï¼Œprompt-tuning åœ¨æŸäº›å°‘è§çš„åˆ†ç±»ä»»åŠ¡ä¸­å–å¾—äº†å¯å–œçš„æˆæœã€‚prompt-tuning çš„æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨è¾“å…¥ä¸­æ’å…¥æ–‡æœ¬ç‰‡æ®µï¼ˆå³æ¨¡æ¿ï¼‰ï¼Œå¹¶å°†åˆ†ç±»ä»»åŠ¡è½¬åŒ–ä¸ºä¸€ä¸ªæ©ç çš„è¯­è¨€å»ºæ¨¡é—®é¢˜ï¼ˆmasked language modeling problemï¼‰ã€‚ç„¶è€Œï¼Œå¯¹äºå…³ç³»æå–æ¥è¯´ï¼Œç¡®å®šä¸€ä¸ªåˆé€‚çš„æç¤ºæ¨¡æ¿éœ€è¦é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼Œè€Œä¸”è¦è·å¾—ä¸€ä¸ªåˆé€‚çš„æ ‡ç­¾è¯æ˜¯å¾ˆéº»çƒ¦å’Œè€—æ—¶çš„ã€‚**æ­¤å¤–ï¼Œå®ä½“å’Œå…³ç³»ä¹‹é—´å­˜åœ¨ç€ä¸°å¯Œçš„è¯­ä¹‰çŸ¥è¯†ï¼ˆå…³ç³»æ ‡ç­¾ä¸­å­˜åœ¨ä¸°å¯Œçš„è¯­ä¹‰å’Œå…ˆéªŒçŸ¥è¯†ï¼‰ï¼Œä¸èƒ½è¢«å¿½è§†ã€‚**ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä¸“æ³¨äºå°†çŸ¥è¯†çº³å…¥å…³ç³»æå–çš„prompt-tuning ä¸­ï¼Œå¹¶æå‡ºäº†ä¸€ç§å…·æœ‰**ååŒä¼˜åŒ–åŠŸèƒ½**çš„**çŸ¥è¯†æ„ŸçŸ¥** prompt-tuning è°ƒæ•´æ–¹æ³•ï¼ˆKnowPromptï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†å®ä½“å’Œå…³ç³»çŸ¥è¯†æ³¨å…¥åˆ°å…·æœ‰å¯å­¦ä¹ çš„è™šæ‹Ÿæ¨¡æ¿è¯ä»¥åŠç­”æ¡ˆè¯çš„ prompt æ„å»ºä¸­ï¼Œå¹¶é€šè¿‡çŸ¥è¯†çº¦æŸæ¥ååŒä¼˜åŒ–å®ƒä»¬çš„è¡¨è¿°ã€‚åœ¨äº”ä¸ªæ ‡å‡†å’Œä½èµ„æºè®¾ç½®çš„æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚https://github.com/zjunlp/KnowPrompt

![å›¾1ï¼šé€šè¿‡å°†ç‰¹å®šä»»åŠ¡æ ¼å¼åŒ–ä¸ºå®Œå½¢å¡«ç©ºä»»åŠ¡æ¥åˆºæ¿€PLMçŸ¥è¯†çš„å³æ—¶è°ƒä¼˜ç¤ºä¾‹ã€‚è™šçƒä¸­çš„På’ŒCä»£è¡¨è¯­ä¹‰å®Œå½¢çš„è™šè¯Personå’ŒCountryã€‚](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20220307133606.png)

å›¾1ï¼šé€šè¿‡å°†ç‰¹å®šä»»åŠ¡æ ¼å¼åŒ–ä¸ºå®Œå½¢å¡«ç©ºä»»åŠ¡æ¥åˆºæ¿€PLMçŸ¥è¯†çš„å³æ—¶è°ƒä¼˜ç¤ºä¾‹ã€‚è™šçƒä¸­çš„På’ŒCä»£è¡¨è¯­ä¹‰å®Œå½¢çš„è™šè¯Personå’ŒCountryã€‚

![å›¾2ï¼šè®ºæ–‡ä¸­æè¿°çš„ç­”æ¡ˆè¯æ˜¯æŒ‡æˆ‘ä»¬æå‡ºçš„è™šæ‹Ÿç­”æ¡ˆè¯ã€‚](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20220307133602.png)

å›¾2ï¼šè®ºæ–‡ä¸­æè¿°çš„ç­”æ¡ˆè¯æ˜¯æŒ‡æˆ‘ä»¬æå‡ºçš„è™šæ‹Ÿç­”æ¡ˆè¯ã€‚

### ç®€å•è®°å½•ï¼š

ç›®å‰çš„çŸ¥è¯†æå–çš„æ–¹æ³•ä¸è¡Œï¼ˆPLMs + classifierï¼‰their performance heavily depends on time-consuming and labor-intensive annotated data, making it hard to generalize well.

ç°åœ¨æœ‰ä¸ªå« Prompt-Tuning çš„æ–¹æ³•è¿˜ä¸é”™ï¼›In a nutshell, prompt-tuning involves template engineering and verbalizer engineering, which aims to search for the best template and an answer space.

ä½†æ˜¯æƒ³ç”¨åœ¨çŸ¥è¯†æå–ä¸Šé¢è¿˜æœ‰å‡ ä¸ªé—®é¢˜ï¼Œ**é¦–å…ˆ**ï¼Œæ‰‹åŠ¨æ„å»ºè´¹æ—¶è´¹åŠ›ï¼ˆdetermining the appropriate prompt template for RE requires domain expertiseï¼‰ï¼Œè‡ªåŠ¨æ„å»ºè®¡ç®—å¼€é”€å¤ªå¤§ï¼ˆauto-constructing a high-performing prompt with input entities often requires additional computation cost for generation and verificationï¼‰ã€‚**å…¶æ¬¡**ï¼Œæƒ³ä»è¾“å‡ºç©ºé—´ä¸­å¾—åˆ°ç›®æ ‡æ ‡ç­¾éš¾åº¦å¤ªå¤§ï¼ˆæœç´¢ç©ºé—´å¤æ‚åº¦ã€ç‰¹å®šæ ‡ç­¾ä¸å­˜åœ¨ï¼‰the computational complexity of the label word search process is very high when the length of the relation label varies, and it is non-trivial to obtain a suitable target label word in the vocabulary to represent the specific relation label. **å¦å¤–**ï¼Œå…³ç³»æ ‡ç­¾ä¸­ã€ä¸‰å…ƒç»„ä¹‹é—´éƒ½æœ‰å…³è”çš„çŸ¥è¯†ï¼ˆå…³ç³»æ ‡ç­¾ä¸­è¯­ä¹‰ã€ä¸‰å…ƒç»„ç»“æ„ç­‰ï¼‰In addition, there exists rich semantic knowledge among relation labels and structural knowledge implications among relational triples, which cannot be ignored.

æ‰€ä»¥ï¼Œä½œè€…å°±å°†çŸ¥è¯†æ³¨å…¥åˆ°å¯å­¦ä¹ çš„ prompts ä¸­å¹¶æå‡ºæ–°çš„çŸ¥è¯†æ„ŸçŸ¥ååŒä¼˜åŒ– Prompt-tunningï¼ˆKnowPromptï¼‰

ç¬¬äºŒç« ï¼Œç®€è¿°å‘å±•å†ç¨‹ï¼Œå…³ç³»æå–è¿‡æ¸¡äº†CNN/RNNæ—¶ä»£ã€åŸºäºå›¾çš„æ–¹æ³•ã€é¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºä¸»å¹²ã€çŸ¥è¯†å¢å¼ºçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ç­‰ï¼›ç„¶åé¡ºä¾¿æäº†ä¸€å˜´ few-shot çš„å†…å®¹ã€‚

ä¹‹åä»‹ç»Prompt-tuningçš„å†…å®¹ï¼Œè¢«GPT-3æ¨åŠ¨è¯ç”Ÿï¼Œç„¶ååœ¨REé¢†åŸŸæœ‰äººæå‡ºäº†[**PTR**](https://arxiv.org/abs/2105.11259)(**Prompt Tuning with Rules for Text Classification**)ï¼Œä¹Ÿä¸çŸ¥é“æ˜¯å•¥ï¼Œå°±è¯´è‡ªå·±çš„å‡ ç‚¹ä¸ä¸€æ ·ã€‚

1. **é¦–å…ˆæå‡ºç”¨è™šæ‹Ÿç­”æ¡ˆè¯æ¥è¡¨ç¤ºç‰¹å®šçš„å…³ç³»æ ‡ç­¾**ï¼Œè€Œä¸æ˜¯PTRä¸­çš„å¤šä¸ªsub-promptã€‚æœ¬è´¨ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¯æ¨¡å‹ä¸å¯çŸ¥çš„ï¼Œå¯ä»¥åº”ç”¨äºç”Ÿæˆå¼è¯­è¨€æ¨¡å‹ï¼Œè€ŒPTRç”±äºå…¶sub-promptæœºåˆ¶è€Œå¤±è´¥ã€‚
2. **å…¶æ¬¡è™šæ‹Ÿè¯éƒ½æ˜¯å¯å­¦ä¹ çš„**ï¼Œå‡å°‘åŠ³åŠ¨åŠ›ä¸”æ›´åŠ çµæ´»å¯æ¨å¹¿ã€‚Secondly, we construct prompt with knowledge injection via learnable virtual type words and virtual answer words to alleviate labor-intensive prompt engineering rather than predefined rules; thus, our method is more flexible and can generalize to different RE datasets easily.
3. **ç¬¬ä¸‰ç»™è¾“å‡ºæ·»åŠ äº†çŸ¥è¯†çº¦æŸå’Œå…³è”**ã€‚Thirdly, we synergistically optimize virtual type words and answer words with knowledge constraints and associate prompt embeddings with each other.

ç¬¬ä¸‰ç« è¡¥å……äº†prompt-tuningçš„èƒŒæ™¯çŸ¥è¯†ï¼Œå¯ä»¥å€ŸåŠ©æ­¤å›¾ç†è§£[[Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing](https://www.notion.so/Pre-train-prompt-and-predict-A-systematic-survey-of-prompting-methods-in-natural-language-process-299045f92e424f64ab7ccd1397b790a3) ]

**çŸ¥è¯†æ³¨å…¥**

åˆ†ä¸ºå®ä½“çŸ¥è¯†æ³¨å…¥å’Œå…³ç³»çŸ¥è¯†æ³¨å…¥ï¼›

![è¡¨1ï¼šæ•°æ®é›†ç¤ºä¾‹](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20220307133502.png)

è¡¨1ï¼šæ•°æ®é›†ç¤ºä¾‹

**å®ä½“çŸ¥è¯†æ³¨å…¥**ï¼šå®ä½“çš„ç±»å‹æ ‡ç­¾æœ‰ç”¨ä½†æ˜¯æ•°æ®é›†ä¸­ä¸ä¸€å®šä¼šæœ‰ï¼Œä½†æ˜¯å¯ä»¥é€šè¿‡ç‰¹å®šå…³ç³»ä¸­åŒ…å«çš„å…ˆéªŒçŸ¥è¯†æ¥è·å¾—**æ½œåœ¨çš„å®ä½“ç±»å‹çš„èŒƒå›´**ï¼ˆHowever, we can obtain the scope of the potential entity types with prior knowledge contained in a specific relation, rather than annotation.ï¼‰ã€‚å¯ä»¥æ ¹æ®å…³ç³»ç±»åæ¥ä¼°è®¡æ½œåœ¨å®ä½“ç±»å‹çš„å€™é€‰é›†çš„å…ˆéªŒåˆ†å¸ƒï¼ˆIntuitively, we estimate the prior distributions $\phi_{sub}$ and $\phi_{obj}$ over the candidate set $C_{sub}$ and $C_{obj}$ of potential entity types, respectively, according to the relation class, where the prior distributions are estimated by frequency statistics.ï¼‰ã€‚æ‰€ä»¥çŸ¥è¯†æ³¨å…¥è¿‡ç¨‹å¯ä»¥å…¬å¼åŒ–ä¸º[å…¬å¼1](https://www.notion.so/KnowPrompt-Knowledge-aware-Prompt-tuning-with-Synergistic-Optimization-for-Relation-Extraction-ab64d0b0c7cf46c49923b118266b59e0)ã€‚$I$ è¡¨ç¤ºå»é‡æ“ä½œï¼Œ$\mathbf{e}$ è¡¨ç¤ºPLMçš„word-embeddingã€‚è¿™å°±å¾—åˆ°äº†å®ä½“çš„ Typer Markerï¼Œå…·ä½“çš„ä½¿ç”¨æ–¹æ³•ç±»ä¼¼äº[å›¾2](https://www.notion.so/KnowPrompt-Knowledge-aware-Prompt-tuning-with-Synergistic-Optimization-for-Relation-Extraction-ab64d0b0c7cf46c49923b118266b59e0)ä¸­çš„ç»¿è‰²éƒ¨åˆ†ã€‚

$$
\begin{equation}\hat{\mathbf{e}}_{[s u b]}=\sum \phi_{s u b} \cdot \mathbf{e}\left(I\left(\mathbf{C}_{s u b}\right)\right)\end{equation}

$$

**å…³ç³»çŸ¥è¯†çš„æ³¨å…¥**ï¼šä»¥å¾€æ˜¯è‡ªåŠ¨ç”Ÿæˆè¯æ±‡è¡¨ä¸­çš„æ ‡ç­¾è¯å’Œä¸€ä¸ªä»»åŠ¡æ ‡ç­¾ä¹‹é—´çš„ä¸€ä¸€æ˜ å°„ï¼Œæœç´¢è®¡ç®—å¤æ‚åº¦å¤§ä¸”æ²¡æœ‰ç”¨åˆ°å…³ç³»çš„è¯­ä¹‰çŸ¥è¯†ã€‚è¿™é‡Œçš„ $\mathbf{e}$ è¡¨ç¤º PLM çš„ HEAD Layer çš„ä¸€ä¸ªé¢å¤–çš„å¯å­¦ä¹ å…³ç³»åµŒå…¥å±‚ã€‚ä½œè€…æè®®å¯¹æœ‰å…³æ ‡ç­¾çš„è¯­ä¹‰çŸ¥è¯†è¿›è¡Œç¼–ç å¹¶ä¿ƒè¿› RE çš„è¿‡ç¨‹ï¼ˆWe propose to encodes semantic knowledge about the label and facilitates the process of REï¼‰ã€‚

$$
\begin{equation}\hat{\mathbf{e}}_{[r e l]}\left(v^{\prime}\right)=\phi_{r} \cdot \mathbf{e}\left(\mathbf{C}_{r}\right)\end{equation}

$$

```python
word_embeddings = self.model.get_input_embeddings() # A torch module mapping vocabulary to hidden states. Embedding(50300, 1024)
# IDs are assigned here
continous_label_word = [a[0] for a in self.tokenizer([f"[class{i}]" for i in range(1, num_labels+1)], add_special_tokens=False)['input_ids']]

# init_answer_words:
for i, idx in enumerate(label_word_idx): # idx: e.g. tensor([265, 138, 18727, 0, 0, 0])
    word_embeddings.weight[continous_label_word[i]] = torch.mean(word_embeddings.weight[idx], dim=0) # mean

# init type words
so_word = [a[0] for a in self.tokenizer(["[obj]","[sub]"], add_special_tokens=False)['input_ids']] # e.g. [50294, 50293]
meaning_word = [a[0] for a in self.tokenizer(
				["person","organization", "location", "date", "country"], add_special_tokens=False)['input_ids']] # e.g. [5970, 17247, 41829, 10672, 12659]

for i, idx in enumerate(so_word):
    word_embeddings.weight[so_word[i]] = torch.mean(word_embeddings.weight[meaning_word], dim=0) # mean, sub = obj
```

**ååŒä¼˜åŒ–**

ç”±äºå®ä½“ç±»å‹å’Œå…³ç³»æ ‡ç­¾ä¹‹é—´å­˜åœ¨å¯†åˆ‡çš„äº¤äº’å’Œè”ç³»ï¼Œå¹¶ä¸”è¿™äº›è™šæ‹Ÿç±»å‹è¯ï¼ˆvirtual type wordsï¼‰ä»¥åŠç­”æ¡ˆè¯ï¼ˆvirtual answer wordsï¼‰éƒ½åº”è¯¥ä¸å‘¨å›´çš„ä¸Šä¸‹æ–‡ç›¸å…³è”ï¼Œå› æ­¤æˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ç§å¯¹è™šæ‹Ÿç±»å‹å‚æ•°é›†ï¼ˆ$\hat{\mathbf{e}}_{[s u b]},\hat{\mathbf{e}}_{[obj]}, \hat{\mathbf{e}}_{[rel]}(V^{'})$ï¼‰å…·æœ‰éšå¼ç»“æ„çº¦æŸçš„ååŒä¼˜åŒ– virtual type words and virtual answer words.

ç¬¬ä¸€ä¸ªçº¦æŸï¼Œä¸Šä¸‹æ–‡æ„ŸçŸ¥ Prompt æ ¡å‡†ï¼ˆContext-aware Prompt Calibrationï¼‰ã€‚åŸºäºè‡ªç„¶è¯­è¨€åˆå§‹åŒ–çš„å‘é‡è¡¨è¾¾å¹¶ä¸ä¸€å®šæ˜¯æœ€ä¼˜çš„ï¼Œæ‰€ä»¥éœ€è¦é€šè¿‡ä¸Šä¸‹æ–‡è¿›ä¸€æ­¥æ ¡å‡†ä»–ä»¬çš„è¡¨ç¤ºï¼ˆAlthough our virtual type and answer words are initialized based on knowledge, they may not be optimal in the latent variable space. They should be associated with the surrounding context. Thus, further optimization is necessary by perceiving the context to calibrate their representation.ï¼‰$p(y \mid x)=p\left([\text { MASK }]=\mathcal{V}^{\prime} \mid x_{\text {prompt }}\right)$ é€šè¿‡å‡å°‘è¿™ä¸ªæŸå¤±æ¥ä¿®æ­£ï¼ˆï¼Ÿï¼‰

$$
\begin{equation}\mathcal{J}_{[\text {MASK }]}=-\frac{1}{|\mathcal{X}|} \sum_{x \in \mathcal{X}} \mathrm{y} \log p(y \mid x)\end{equation}

$$

ç¬¬äºŒä¸ªçº¦æŸï¼Œéšå¼ç»“æ„çº¦æŸï¼ˆImplicit Structured Constraintsï¼‰ï¼ˆå³ä»£ç ä¸­çš„KE-Lossï¼‰ã€‚$(s,r,o)$ åˆ†åˆ«è¡¨ç¤ºï¼ˆvirtual types of subject, virtual types of object, relation labelï¼‰ä»–ä»¬çš„è¯åµŒå…¥éƒ½æ˜¯ç›´æ¥å°† virtual type words and virtual answer words çš„è¾“å‡ºæ¥è®¡ç®—çš„ï¼Œç»“æ„æŸå¤±å¦‚å…¬å¼æ‰€ç¤ºï¼Œç­‰å¼çš„ååŠéƒ¨åˆ†æ˜¯è´Ÿä¾‹æ ·æœ¬ï¼ˆ$(s_i^{'}, r, o_i^{'})$ are negative samples,  $\gamma$  is the margin, $\sigma$ refers to the sigmoid function and $d_r$ is the scoring function.ï¼‰

$$
\begin{equation}
\begin{aligned}
&\mathcal{J}_{\text {structured }}=-\log \sigma\left(\gamma-d_{r}(\mathrm{~s}, \mathbf{o})\right) 
-\sum_{i=1}^{n} \frac{1}{n} \log \sigma\left(d_{r}\left(\mathrm{~s}_{\mathbf{i}}^{\prime}, \mathbf{o}_{\mathbf{i}}^{\prime}\right)-\gamma\right)
\end{aligned}
\end{equation}

$$

$$
\begin{equation}d_{r}(\mathbf{s}, \mathbf{o})=\|\mathbf{s}+\mathbf{r}-\mathbf{o}\|_{2}\end{equation}

$$

### å®éªŒç»“æœä»¥åŠç»“è®ºï¼š

![Untitled](https://xerrors.oss-cn-shanghai.aliyuncs.com/imgs/20220307133445.png)

<aside>
ğŸ’¡ ç»“è®ºï¼šIn this paper, we present KnowPrompt for relation extraction, which mainly includes knowledge-aware prompt construction and synergistic optimization with knowledge constraints. In the future, we plan to explore two directions, including: (i) extending to semi-supervised setting to further leverage unlabeled data; (ii) extending to lifelong learning, whereas prompt should be optimized with adaptive tasks. ï¼ˆæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºçŸ¥è¯†æ„ŸçŸ¥çš„å…³ç³»æŠ½å–æ–¹æ³•KnowPromptï¼Œä¸»è¦åŒ…æ‹¬çŸ¥è¯†æ„ŸçŸ¥çš„æç¤ºç¬¦æ„é€ å’ŒåŸºäºçŸ¥è¯†çº¦æŸçš„synergistic ä¼˜åŒ–ã€‚æœªæ¥ï¼Œæˆ‘ä»¬è®¡åˆ’æ¢ç´¢ä¸¤ä¸ªæ–¹å‘ï¼ŒåŒ…æ‹¬:(i)æ‰©å±•åˆ°åŠç›‘ç£è®¾ç½®ï¼Œè¿›ä¸€æ­¥åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®;(2)æ‰©å±•åˆ°ç»ˆèº«å­¦ä¹ ï¼Œè€Œæç¤ºåº”é€šè¿‡é€‚åº”æ€§ä»»åŠ¡ä¼˜åŒ–ã€‚ï¼‰
</aside>
